# 笔记2

## 内容总结





##  **补充：矩阵乘法的计算复杂度**

假设有两个矩阵：
- **矩阵 A**：维度为 $(m \times n)$
- **矩阵 B**：维度为 $(n \times p)$

那么 **矩阵乘法 $C = A \times B$** 的计算复杂度是多少？

矩阵乘法的定义如下：
$$C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}$$
- 每个元素 $C_{ij}$ 需要 **n 次乘法和 n-1 次加法**。
- **C 矩阵的维度为 $(m \times p)$**，所以总共需要计算 **$ m \times p $ 个元素**。
- **乘法次数**：$m \times p \times n$
- **加法次数**：$m \times p \times (n - 1)$ （通常忽略加法，重点分析乘法）
- **时间复杂度**：$O(mnp)$

### **特殊情况**
- **方阵乘法**（如果 $m = n = p$）：计算复杂度为 **$ O(n^3) $**。
- **向量点积**（如果 $m = p = 1$）：计算复杂度为 **$ O(n) $**。


### **复杂度优化**
经典矩阵乘法为 **$O(mnp)$**，但在特殊情况下，可以使用更快的算法：
- **Strassen 算法**（适用于方阵，降低到 **$O(n^{2.81})$**）
- **Coppersmith-Winograd 算法**（目前最优 **$O(n^{2.37})$**）
- **GPU 并行计算**（利用批量矩阵运算加速）

---

### **矩阵乘法计算复杂度总结**
| 乘法类型 | 计算复杂度 |
|----------|------------|
| 一般矩阵乘法 ($m \times n $ 和 $ n \times p$) | $O(mnp)$ |
| 方阵乘法 ($n \times n$) | $O(n^3)$ |
| Strassen 算法 | $O(n^{2.81})$ |
| Coppersmith-Winograd 算法 | $O(n^{2.37})$ |
| 向量点积 ($d$ 维向量) | $O(d)$ |

**另外：尽管有些时候数学计算复杂度相同，我们还需要考虑内存访问的复杂度，这里往往会有几倍的差距**<p>
以计算 $A^TB$和 $AB^T$为例子。计算 $A^TB$ 需要额外存储 $A^T$ ，大规模运算时会占用更多的内存，计算 $AB^T$ 则不需要。

---

## 补充：位置编码的实现

### 为什么需要位置编码？
在传统的序列模型（如 RNN、LSTM）中，序列的顺序信息是通过时间步（time step）隐式地传递。而 Transformer 抛弃了这种递归结构，完全依赖**自注意力机制**来捕捉全局依赖关系。然而，自注意力机制本身无法区分序列中不同位置的元素，因此需要**显式**地引入位置信息。显然在NLP任务中，单词的顺序至关重要，例如：
- a cat is there
- is there a cat


### 位置编码的计算
Google在论文 [Attention is all you need](https://arxiv.org/abs/1706.03762) 中提到，他们比较过直接训练出来的位置向量和下面公式计算出来的位置向量，两者效果是接近的。我们这里选用公式构造的Position Embedding了，**优点**是不需要训练参数，而且即使在训练集中没有出现过的句子长度上也能用。<p>
Positional Encoding的公式如下：

$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

其中， $pos$ 表示单词在序列中的位置（从零开始）， $2i$ 和 $2i+1$ 表示维度索引（从 $1$ 到 $ d_{model}$ ）， $d_{model}$ 表示词向量的维度（embedding size）， $10000$ 一个比较大的常数用于放缩。



### 为什么选用正余弦函数？

1. 不同位置的编码不会重复：由于不同的位置 $pos$ 会产生不同的 $PE$ 值，因此不同位置的单词编码都是唯一的。<p>
2. 可推广到不同长度的序列：
- 无需训练额外参数（不像可学习的位置编码）。
- 可以外推（extrapolate）到比训练时更长的序列。<p>
3. 周期性和距离信息：
- 正弦和余弦函数具有周期性，可以帮助模型学习相对位置关系。
- 频率随维度指数级变化，确保不同尺度的信息都能被学习。<p>




## 问题解答
### 1. 什么是软注意力和硬注意力？<p>
- **软注意力（Soft Attention）** 是一种平滑、连续的机制，适用于大多数需要在多个输入元素之间进行加权计算的任务，且可以通过标准的梯度下降方法优化。在软注意力机制中，模型对**所有输入**进行计算，给每个输入分配一个权重（即注意力权重）。<p>
- **硬注意力（Hard Attention）** 是一种离散的、非可微的机制，通常用于那些需要明确选择某些输入元素的任务，但由于其不可微的特性，训练过程更加复杂，需要强化学习等方法来进行优化。硬注意力在处理时**选择**某些输入元素，直接“关注”它们，而忽略其他输入。<p>
一言以蔽之，硬注意力在每一步操作时，只选择一个或少数几个特定的输入元素，而软注意力对所有输入进行加权平均。详情看[软注意力和硬注意力](https://github.com/RINZERON/fun-transformer/blob/main/docs/chapter2/%E8%BD%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E7%A1%AC%E6%B3%A8%E6%84%8F%E5%8A%9B.md)


### 2. 什么是“自注意力”（Self-Attention）机制？<p>

一句话总结：
- 自注意力是在**序列内部**进行注意力运算，旨在寻找序列内部的联系。<p>

其原理可进一步解释为：注意力运算的输入是 $Q(Query), K(Key), V(Value)$ ，对于**自注意力**而言，是对同一个输入序列 $X$，分别进行三种独立的线性变换得到 $Q_X、K_X、V_X$ 后，将其作为输入并进行注意力运算，体现在公式上即 $\text{Attention} (Q_X, K_X, V_X)$。


### 3. “掩码”多头自注意力有什么用？

掩码（Mask）操作是针对Decoder，掩盖 $t$ 时刻之后的输出。
目的是让 $q_t$ ，只能识别到 $k_1$ 到 $k_{t-1}$ ，这些 $QK$ 配对才有被赋予权重的意义，后面的 $QK$ 配对权重均为0。因为Decoder在做预测的时候，我们并不能知道 $t$ 时刻之后的输出值。Mask操作保证了预测和训练的结果是一致的。

另外我们还有填充掩码（Padding Mask），目的是将无谓的填充值掩盖，以免影响模型输出。



### 4. 什么是受限的自注意力运算（restricted Self-Attention）?

就是设定窗口的自注意力运算，跟局部注意力机制类似。计算 Attention 时仅考虑每个输出位置附近窗口的 r 个输入。这带来两个效果：计算复杂度降为 $O(r \times n \times d)$ 最长学习距离降低为 $r$，因此需要执行 $O(\frac{n}{r})$ 次才能覆盖到所有输入。（考虑 $n$ 个 key 和 $n$ 个 query 两两点乘，维数为 $d$ ）


### 5. **先安装python，移植到conda环境中可能出现pip install location位置不对，导致包加载出错，可能路径依赖问题？怎么解决？**

重新用Conda创造一个干净的环境，做为项目的专用python环境。
在CMD中输入，
```bash
conda create --name env_name python=3.9/3.8...
```

之后安装相应的库。（记得在对应的环境中）
```bash
pip install package_name<p>
conda install  package_name
```

**注意**：在VSCode中的终端选择正确的内核。如果想要在jupyter中安装，则ipynb文件单元格中应该是
```bash
%pip install package_name
```


安装相应库后，要重启jupyter内核，程序才能找到包。

### 6. **numpy与gensim包冲突怎么解决？**
在CMD输入
```bash
pip install --upgrade gensim
```
自动更新gensim包，并且会检查numpy包是否符合要求，卸载并安装合适的numpy包。

在CMD输入
```bash
pip check
No broken requirements found.
```
可查看有无冲突的包。


### 7. **怎么在VSCode中查看jupyter的变量值？**

安装扩展（ctrl+shift+x）Data Wrangler。
VSCode提示安装相应的包，选择确定即可。或者我们自行在终端输入（安装相应的包）。
```bash
pip install package_name
```

Data Wrangler 的核心依赖包包括：
- pandas、numpy：数据处理。
- scikit-learn：机器学习。
- matplotlib、seaborn：数据可视化。
- ipykernel、jupyter：交互式环境支持。

安装这些包后，我们可以在 VSCode 中充分利用 Data Wrangler 的功能进行数据分析和清洗。如果遇到问题，请检查包是否安装正确，并确保 VSCode 使用了正确的 Python 环境。

### 8. **什么是独热编码？**

独热编码是一种将类别型数据（比如单词）转换为计算机可以理解的数字形式的方法。其核心思想是每个单词用很长的向量表示，向量的维度是词汇表的长度，向量只有一个位置为1，其余位置均为0。例如[“猫”，“狗”，“鱼”]对应的独热编码向量为[1,0,0]、[0,1,0]和[0,0,1]。显然独热编码不具备语义信息，只是单纯地表示单词存在，而且会造成维度爆炸等后果。


### 9. **什么是“共现”次数？** 
共现次数是指在一个语料库（文本集合）中，两个单词在一定范围内（比如一个句子或一个固定大小的窗口）同时出现的次数。这个窗口是人为根据训练所需设定的。在 GloVe 等词向量模型中，共现次数被用来训练词向量，使得语义相近的单词在向量空间中距离更近。

### 10. **GloVe模型的缺点有哪些？**
   - 依赖语料库，如果某个词在文本中出现次数太少（比如专业术语或冷门词），GloVe 很难准确学习它的含义，因为模型主要依赖“统计共现次数”，低频词的统计信息不足。
   - 无法处理“一词多义”的情况（每个单词只分配一个向量，太少），无法处理语料库之外的新词，无法处理动态上下文（因为固定窗口）。这些也是静态模型的通病。
   - 缺少词序的信息，例如“狗追猫”和“猫追狗”在共现统计中被视为相同。

### 11. 你好