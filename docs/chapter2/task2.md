# 笔记2





## 问题解答
1. **什么是软注意力和硬注意力？**<p>
- **软注意力（Soft Attention）** 是一种平滑、连续的机制，适用于大多数需要在多个输入元素之间进行加权计算的任务，且可以通过标准的梯度下降方法优化。在软注意力机制中，模型对**所有输入**进行计算，给每个输入分配一个权重（即注意力权重）。<p>
- **硬注意力（Hard Attention）** 是一种离散的、非可微的机制，通常用于那些需要明确选择某些输入元素的任务，但由于其不可微的特性，训练过程更加复杂，需要强化学习等方法来进行优化。硬注意力在处理时**选择**某些输入元素，直接“关注”它们，而忽略其他输入。<p>
一言以蔽之，硬注意力在每一步操作时，只选择一个或少数几个特定的输入元素，而软注意力对所有输入进行加权平均。详情看[软注意力和硬注意力](https://github.com/RINZERON/fun-transformer/blob/main/docs/chapter2/%E8%BD%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E7%A1%AC%E6%B3%A8%E6%84%8F%E5%8A%9B.md)