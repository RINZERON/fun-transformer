# 笔记1

## 补充：RNN、LSTM和GRU<p>参考链接：https://zhuanlan.zhihu.com/p/32085405、https://zhuanlan.zhihu.com/p/32481747


|RNN| LSTM| GRU|
|-------|--------|------|
|循环神经网络<p> Recurrent Neural Network<p>|长短期记忆网络<p>Long Short-Term Memory<p>|门控循环单元<p>Gate Recurrent Unit<p>|
|每一个节点的输入包含两个部分：当前状态的输入 $x^{t}$ 和继承自上一节点的输入 $h^{t-1}$ 。然后计算得到当前状态的输出 $y^{t}$ 和传递给下一节点的输出 $h^{t}$ 。 $h^{t-1}$ 和 $h^{t}$ 承担了记忆的作用。但这种记忆方式比较“呆萌”，在处理长序列时效果不好。|解决传统RNN在长序列训练过程中的问题。LSTM添加了门控装置，当前节点传递给下一节点的输出有两个： $c^{t}$ （变化较小，长时记忆）和 $h^{t}$ （变化较大，短时记忆）。通过门控装置计算这两个输出，LSTM可以选择性地遗忘上一节点内容和选择性地记忆当前状态的内容。|同样是添加了门控装置来解决长序列训练问题。相比于LSTM的两个门控，GRU只有一个门控，计算更加简单，训练成本更低。|

## 补充：通信原理的概念<p>
通信的基本流程包括以下几个步骤:
1. 信号的产生：将原始信息（如声音、文字）转换为电磁信号。<p>
2. 信号的调制：为了适应信道传输，信号需要经过调制（Modulation），将其特性（如幅度、频率、相位）改变。<p>
3. 信号的传输：通过信道（如有线或无线）将信号发送到接收端。<p>
4. 信号的接收：接收器接收信号，并进行解调（Demodulation），还原原始信号。<p>
5. 信号的处理：将还原后的信号转换为用户可以理解的信息。<p>

在电子通信领域，信号的调制和解调本质上就是信息的**编码**与**解码**。<p>

## 补充：激活函数（ReLU，tanh，Sigmoid）<p>



## 补充：Encoder-Decoder 的缺陷
Encoder（编码器）和 Decoder（解码器）之间只有一个固定长度的“上下文向量”来传递信息。为了便于理解，我们类比为“压缩-解压”的过程：<p>
将一张 800X800 像素的图片压缩成 100KB，看上去还比较清晰。再将一张 3000X3000 像素的图片也压缩到 100KB，看上去就模糊了（尤其是图片中的配文）。
    
![图片描述](./images/CH1-image8.png)
    
Encoder-Decoder 就是面临类似的问题：当输入信息太长时，会丢失掉一些信息。传统RNN的压缩信息过程中是无意识、“无选择的”（选择方式比较直接、呆萌）。比如这张图片的文字是关键信息，屏幕、衣服等信息则相对无用。在压缩过程中，衣服、屏幕占据图片的大部，得以清晰地保留，文字作为小部则模糊了。LSTM、GRU尽管通过门控机制解决了长序列信息训练的部分问题，但在选择性遗忘、保留信息，可能会造成细节关键信息的丢失。相关研究也有从这里下手，对**信息预处理**或者对**Decoder提示**等等。

### 论文一览
|操作|提示Decoder<p>（指针）|提示Decoder<p>（注意力）|提示Decoder<p>（强化深度学习）|
|-----|----|-----|-----|
|论文链接|[Pointer Sentinel Mixture Models](https://arxiv.org/abs/1609.07843)|[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)|[Filter based Taxonomy Modification for Improving Hierarchical Classification](https://arxiv.org/abs/1603.00772)|
|核心贡献|论文在 LSTM 语言模型中引入“指针-哨兵”混合机制，通过外部记忆提示（Pointer）增强对长文本上下文的编码能力。通过指针机制，Decoder 可以直接从输入序列的元素中进行选择，而不仅仅是生成新的词汇。|论文介绍了 Seq2Seq 模型与注意力机制，特别是在 Decoder 阶段如何通过注意力机制提示模型关注输入序列的特定部分。虽然论文的重点是注意力机制，但它也深入探讨了如何优化 Decoder 阶段，使其能更加有效地利用信息。|论文探讨了将深度强化学习 (DRL) 引入 Seq2Seq 模型的应用，尤其是如何在 Decoder 阶段引入额外的提示信息。通过强化学习，模型能够在训练过程中自我调整和优化生成策略，这也为 Decoder 提供了额外的策略性提示，使得解码过程更加灵活和高效。|


## 问题解答<p>

1. **填充的元素如何选择？**<p>
填充的元素我们可以自行假定，一般采用0，因为0在大多数情况下不会对模型的学习产生明显的影响。但在某些情况下，我们可以使用一些特殊的填充符号（比如*，-1等等），并在训练过程中对这些位置进行特殊处理。<p>
2. **细粒度细节知识是什么？（行业黑话）**<p>
在计算机科学中，“细粒度细节”（Fine-grained details）通常指系统中更具体、更底层或更小粒度的组成部分或实现细节。与之相对的是“粗粒度”（Coarse-grained），后者指更大、更抽象的模块或功能。
在深度学习领域，细粒度细节通常指模型中更具体、更底层或更小粒度的技术实现、超参数选择或数据/特征处理策略。
在翻译任务中，输入序列的细粒度细节指的是一个个的单词，粗粒度则指的是一个长段的句子。
3. **为什么RNN处理长序列的能力不行？**<p>
循环神经网络的输出层通常包含一系列的神经元连接，这些链接能够通过参数化的方式在短时记忆中储存信息。因此，在长序列处理过程中，传统的RNN需要**额外的计算资源**（如存储和访问数据集）来确保长期依赖性的有效传递。
4. **什么是交叉熵损失？**<p>
交叉熵损失函数是机器学习中常用于分类任务的一种损失函数，它衡量的是模型预测的概率分布与真实标签的概率分布之间的差异。计算公式如下： $$H(P, Q) = -\Sigma( P(i) * \log( Q(i)))$$ 其中，P代表真实的概率分布，Q代表模型预测的概率分布，i代表类别的索引。当模型的预测完全准确时，交叉熵损失达到最小值0。在PyTorch中，交叉熵损失的计算可以通过CrossEntropyLoss类实现，该类自动执行softmax操作和对数似然损失（Negative Log Likelihood Loss）的计算